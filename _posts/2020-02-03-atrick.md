---
layout: post
title: "A Trick for Rapid Iteration with Spark"
date: 2020-02-03 8:00
author: pwais
---

<center><img src="{{site.baseurl}}/assets/images/spark_ship_libs.png" width="90%" style="border-radius: 8px; border: 8px solid #ddd;" /></center>

## ETL and Spark

Data wrangling is a key component of any machine learning or data science pipeline.  These pipelines <b>e</b>xtract raw data from logs or databases, <b>t</b>ransform the data into some desired format, and then <b>l</b>oad the results into a new data store or directly into a model.  ETL might be as little as a line of code or as much as a full-fledged software library.  Indeed, ETL is a key source of [technical debt](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf) in machine learning systems; moreover, the success of project can sometimes depend on [how quickly engineers can iterate](http://www.catb.org/~esr/writings/cathedral-bazaar/cathedral-bazaar/ar01s04.html) on ETL code and leverage data available to them.  

[Spark](https://spark.apache.org/) is one of many tools for ETL (and might be [one of the best]({{site.baseurl}}{% post_url 2020-02-03-etl-spark-vs %})).  Spark is popularly known as distributed computing tool, as it is one of several open source implementations of the [MapReduce paradigm](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf).  Indeed, parallel computing is key for ETL: executing a job on 10 cores often makes the job 10x faster.  

One differentiator between Spark and other tools is that Spark supports *interactive* ETL (e.g. in [Jupyter](https://jupyter.org/) notebooks) via [cloudpickle](https://github.com/cloudpipe/cloudpickle), an advanced code-centric `pickle` library for Python.  The `cloudpickle` package supports serializing `lambda` functions, code defined in the `__main__` module (e.g. a Jupyter notebook kernel or Python interpreter session), and dynamically generated (sub)classes.  When you run a Spark program, part of your code is serialized via `cloudpickle` in the local driver Python process, sent to (likely remote) worker Python processes, deserialized there and run.  

But what if you want to use a library of semi-static Python code in your job?  What if you want to make local changes to that library (e.g. pre-commit modifications) and run that modified code in your Spark job?  Spark provides a [`pyFiles`](https://spark.apache.org/docs/latest/configuration.html#runtime-environment) configuration option for including Python Egg(s) with a job.  (Spark even provides caching and torrent-based distribution for these per-job data files).  However, traditional use of the `pyFiles` feature requires user configuration and/or a custom build step.  This article demonstrates how to *automatically* include with a Spark job the library containing the calling code using `oarphpy`.  


## Automatic User Library Inclusion in Spark

(The below has been tested with a b c 1 2 3 



---------





---------















<script src="http://gist-it.appspot.com/https://github.com/apache/spark/blob/a131031f95dbd426516b88e1dac38965351eb501/python/pyspark/sql/utils.py?slice=24:35" />


GitHub:
<script src="https://gist.github.com/pwais/b744be6b3e8290377ab882a753d5d552.js"></script>

`oarphpy.spark` provides a utility that does the following:
 * 



[link](test2.md)

```
block
```

```python
def foo():
  return 5
```

```c++
struct moof {
  int a;
};
```

<table>
    <tr>
        <td>Foo</td>
    </tr>
</table>


Begin nb html



  <div tabindex="-1" id="notebook" class="border-box-sizing" width="768px">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Spark-DataFrame--&gt;-Tensorflow-Dataset">Spark DataFrame -&gt; Tensorflow Dataset<a class="anchor-link" href="#Spark-DataFrame--&gt;-Tensorflow-Dataset">&#182;</a></h1><p>This notebook serves as a playground for testing <code>oarphpy.spark.spark_df_to_tf_dataset()</code>.  See also the unit tests for this utiltiy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Common imports and setup</span>
<span class="kn">from</span> <span class="nn">oarphpy.spark</span> <span class="k">import</span> <span class="n">NBSpark</span>
<span class="kn">from</span> <span class="nn">oarphpy.spark</span> <span class="k">import</span> <span class="n">spark_df_to_tf_dataset</span>
<span class="kn">from</span> <span class="nn">oarphpy</span> <span class="k">import</span> <span class="n">util</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">Row</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">NBSpark</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/google/protobuf/__init__.py:37: UserWarning: Module oarphpy was already imported from /opt/oarphpy/oarphpy/__init__.py, but /opt/oarphpy/notebooks is being added to sys.path
  __import__(&#39;pkg_resources&#39;).declare_namespace(__name__)
2019-12-27 21:01:03,560	oarph 336 : Trying to auto-resolve path to src root ...
2019-12-27 21:01:03,561	oarph 336 : Using source root /opt/oarphpy 
2019-12-27 21:01:03,589	oarph 336 : Generating egg to /tmp/op_spark_eggs_e2392756-5287-4e0e-bdb3-3bc52ee6cde4 ...
2019-12-27 21:01:03,641	oarph 336 : ... done.  Egg at /tmp/op_spark_eggs_e2392756-5287-4e0e-bdb3-3bc52ee6cde4/oarphpy-0.0.0-py3.6.egg
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Test-on-a-&quot;large&quot;-2GB-random-dataset">Test on a "large" 2GB random dataset<a class="anchor-link" href="#Test-on-a-&quot;large&quot;-2GB-random-dataset">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Create the dataset</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">NUM_RECORDS</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">DATASET_PATH</span> <span class="o">=</span> <span class="s1">&#39;/tmp/spark_df_to_tf_dataset_test_large&#39;</span>
<span class="k">def</span> <span class="nf">gen_data</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
  <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">15</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">Row</span><span class="p">(</span><span class="n">part</span><span class="o">=</span><span class="n">n</span> <span class="o">%</span> <span class="mi">100</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">x</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">NUM_RECORDS</span><span class="p">))</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">gen_data</span><span class="p">))</span>
<span class="k">if</span> <span class="n">util</span><span class="o">.</span><span class="n">missing_or_empty</span><span class="p">(</span><span class="n">DATASET_PATH</span><span class="p">):</span>
    <span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">parquet</span><span class="p">(</span><span class="n">DATASET_PATH</span><span class="p">,</span> <span class="n">partitionBy</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;part&#39;</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>

</div>
</div>

End nb html

<!--
<div style="left: 0px; border: 0px none; height: 370px; position: fixed; width: 270px; overflow: hidden; bottom: -67px;">
    <div style="overflow: hidden;">
    </div>
    <iframe src="http://weather.gc.ca/wxlink/wxlink.html?cityCode=on-162&amp;lang=e" scrolling="no" style="height: 300px; border: 0px none; width: 165px; margin-bottom: 0px; margin-left: 24px;">
    </iframe>
    </div>
    </div> -->


Start iframe
<iframe src='{{site.baseurl}}/assets/Spark_Custom_Library_AutoEgg.html' style="border-radius: 8px; border: 8px solid #ddd; position: absolute; left: 0px; height: 5000px; width: 100%; min-width: 1200px; "  />
End iframe

