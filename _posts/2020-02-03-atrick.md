---
layout: post
title: "A Trick for Rapid Iteration with Spark"
date: 2020-02-03 8:00
author: pwais
---

<center><img src="{{site.baseurl}}/assets/images/spark_ship_libs.png" width="90%" style="border-radius: 8px; border: 8px solid #ddd;" /></center>

## ETL and Spark

Data wrangling is a key component of any machine learning or data science pipeline.  These pipelines **extract** raw data from logs or databases, **transform** the data into some desired format, and then **load** the results into a new data store or directly into a training pipeline.  ETL might be as little as a line of code or as much as a full-fledged software library.  Indeed, ETL is a key source of [technical debt](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf) in machine learning systems; moreover, the success of a project can sometimes depend on [how quickly engineers can iterate](http://www.catb.org/~esr/writings/cathedral-bazaar/cathedral-bazaar/ar01s04.html) on ETL code and leverage data available to them.  

[Spark](https://spark.apache.org/) is one of many tools for ETL (and might be [one of the best]({{site.baseurl}}{% post_url 2020-02-03-etl-spark-vs %})).  Spark is popularly known as a distributed computing tool, as it is one of several open source implementations of the [MapReduce paradigm](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf).  Indeed, parallel computing is key for ETL: executing a job on 10 cores instead of 1 often makes the job 10x faster.  

One differentiator between Spark and traditional MapReduce is that Spark supports **interactive** development of ETL programs in [Jupyter](https://jupyter.org/) or [Google Colab](https://colab.research.google.com/) notebooks.  A key enabler of this feature is [cloudpickle](https://github.com/cloudpipe/cloudpickle), an advanced code-oriented serialization library (versus `pickle`, which is data-oriented).  The `cloudpickle` package supports serializing `lambda` functions, code defined in the `__main__` module (e.g. a Jupyter notebook kernel or Python interpreter session), and even dynamically generated classes.  When you run a Spark program, part of your code is serialized via `cloudpickle` in the local driver Python process, sent to remote worker Python processes, deserialized there and run.  

But what if you want to use a library of your own Python code in your job?  What if you want to make local changes to that library (e.g. pre-commit modifications) and run that modified code in your Spark job?  Spark provides a [`pyFiles`](https://spark.apache.org/docs/latest/configuration.html#runtime-environment) configuration option for including Python Egg(s) with a job.  (Spark even provides caching and torrent-based distribution for these per-job data files).  However, traditional use of the `pyFiles` feature requires user configuration and/or a custom build step.  This article demonstrates how to **automatically** include with a Spark job the library containing the calling code using `oarphpy`.  


Below we embed a demo notebook provided [as part of `oarphpy`](https://github.com/pwais/oarphpy/blob/master/notebooks/Spark_Custom_Library_AutoEgg.ipynb).


<iframe src='{{site.baseurl}}/assets/Spark_Custom_Library_AutoEgg.html' style="border-radius: 8px; border: 8px solid #ddd; position: absolute; left: 0px; height: 5500px; width: 100%; min-width: 1200px; "  />



